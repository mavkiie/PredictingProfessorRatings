# -*- coding: utf-8 -*-
"""Homework4-Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kafxCq_RQZiWg9LUbCEvWINkwGVeD7Oh
"""

#Installing all appropriate packages
!pip install transformers datasets torch pandas scikit-learn matplotlib seaborn requests planet-terp-client

#Imports
import pandas as pd
import numpy as np
import requests
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from datasets import Dataset
from sklearn.metrics import classification_report, confusion_matrix, mean_absolute_error
import matplotlib.pyplot as plt
import seaborn as sns

#Testing API
from planet_terp import PlanetTerp

client = PlanetTerp()

professors = ["Kendall Williams", "Justin Wyss-Gallifent", "Larry Herman", "Fawzi Emad", "Nelson Padua-Perez"]

#Gathering all reviews and organizing in array
#gets all reviews
from planet_terp import PlanetTerp

client = PlanetTerp()
professors = [
    "Kendall Williams",
    "Justin Wyss-Gallifent",
    "Nelson Padua-Perez",
    "Pedram Sadeghian",
    "Fawzi Emad"
]

all_reviews = []

for name in professors:
    try:
        p = client.professor(name, reviews=True)
        reviews = p.reviews
        for r in reviews: #for every review in this professors reviews, append it to the array
            all_reviews.append({
              "professor": name,
              "review_text": r.review,
              "rating": r.rating,
              "course": r.course,
        })
    except Exception as e:
        print(f"Failed to fetch {name}: {e}")

#Cleaning up the data and storing in pandas dataframe
df = pd.DataFrame(all_reviews)
df = df.dropna()
df = df[df["review_text"].str.strip() != ""]
df["label"] = df["rating"] - 1  #replacing with 0-4 labels
df["review_text"] = df["review_text"].str.replace("\n", " ").str.strip()

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 5))  #larger, cleaner figure
sns.set_style("whitegrid")  #clean background

sns.countplot(
    x=df["rating"],
    color="#88E788",  #bar color
    edgecolor="black" #makes bars more defined
)

plt.title("Distribution of Star Ratings", fontsize=16)
plt.xlabel("Rating", fontsize=14)
plt.ylabel("Count", fontsize=14)
plt.tight_layout()
plt.show()

#Setting up training
from sklearn.model_selection import train_test_split
#Splits between training data frame and validation dataframe
#test size is 2, testing on the label column
train_df, val_df = train_test_split(
    df,
    test_size=0.2,
    stratify=df["label"],
    random_state=42
)

from transformers import AutoTokenizer
from torch.utils.data import Dataset, DataLoader
import torch

MAX_LEN = 256
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

class ReviewsDataset(Dataset):
    def __init__(self, df, tokenizer, max_len=256):
        self.texts = df["review_text"].tolist()
        self.labels = df["label"].tolist()
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding="max_length",
            max_length=self.max_len,
            return_tensors="pt"
        )
        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "labels": torch.tensor(label, dtype=torch.long)
        }

train_dataset = ReviewsDataset(train_df, tokenizer, MAX_LEN)
val_dataset = ReviewsDataset(val_df, tokenizer, MAX_LEN)

#TRAIN

from transformers import TrainingArguments, Trainer
import numpy as np
from sklearn.metrics import accuracy_score, f1_score


#load model
model = AutoModelForSequenceClassification.from_pretrained(
    "distilbert-base-uncased",
    num_labels=5
)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# model = model.to(device)


training_args = TrainingArguments(
    output_dir="./results",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    num_train_epochs=3,
    report_to=[]
)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    return {
        "accuracy": accuracy_score(labels, preds),
        "f1_macro": f1_score(labels, preds, average="macro")
    }

trainer = Trainer(
    model= model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

trainer.train()
trainer.evaluate()

#Inferencing. Here the model is given new unseen reviews and asking it to predict a star rating
#Here, we will pass in two simple reviews the model has not seen. 1st: Clyde Kruskal 2nd: Michael Marsh

new_reviews = [
    "SUCH A GOOD TEACHER!! Always has a joyful attitude and is great at going over every detail. 10/10!!",
    "Class wastes time. Projects terrible",
    "Yoon is nice person, but should not be teaching this class especially. Projects are easy but exams are unfair and completely unrelated to any other work we do in the class. Most multiple choice questions are on random niche things that he talked about for 30 seconds and have one bullet point on a random lecture slide. He tests on things beyond the scope of 131, such as sorting algorithms. He also gets into convoluted explanations of topics that do not help with understanding the topic add hand. Avoid yoon if possible, especially for 131. If you have him be sure to go over all the lecture slides before exams/quizzes and do your own research on things that are not fully explained in the slides.",
    "To sum things up concisely, the topics covered in this class aren't bad, but Cliff's lecturing style hurts overall engagement. I could not attend an entire lecture without sleeping or doing other work. Most of my productive learning came from reading Anwar's slides and Cliff's notes and starting the projects early. Exams, on the other hand, were pretty chill. They were very similar to past semester exams posted on the website. The quizzes were meh. As many others have mentioned, the first quiz is generally the worst. After we took the first quiz, my TA said, Congrats, guys. The first quiz historically has a D average. Don't panic about the first one; the others are much better. Overall, three stars for many opportunities to help your grade/do well. Minus two stars for Cliff's lecture engagement."]

inputs = tokenizer(new_reviews, padding=True, truncation=True, return_tensors="pt").to(device)

#Predicts the grades
model.eval()
with torch.no_grad(): #picks class with the highest score
    outputs = model(**inputs) #outputs.logits: the model's rae scores for the 5 rating classes
    predictions = torch.argmax(outputs.logits, dim=-1)

print("Predicted labels:", predictions.cpu().numpy())
#The model predicts 0 and 4 (5 == 4 and 0 == 1 since we are using the label column)

import torch
import numpy as np
from torch.utils.data import DataLoader

#make sure the model is in evaluation mode
model.eval()

preds = []
labels = []
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

with torch.no_grad():
    for batch in val_loader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        label = batch["labels"].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        batch_preds = torch.argmax(outputs.logits, dim=-1)

        preds.extend(batch_preds.cpu().numpy())
        labels.extend(label.cpu().numpy())

#convert to numpy arrays for easier handling
preds = np.array(preds)
labels = np.array(labels)

#print predictions vs actual labels for the first 20 samples
for i in range(min(20, len(preds))):
    print(f"Review: {val_df.iloc[i]['review_text']}")
    print(f"Actual rating: {val_df.iloc[i]['rating']}, Predicted label: {preds[i] + 1}")  # +1 if you mapped ratings 1-5 â†’ 0-4
    print("-" * 80)

from sklearn.metrics import confusion_matrix, classification_report, mean_absolute_error
import seaborn as sns
import matplotlib.pyplot as plt

labels_orig = labels + 1
preds_orig = preds + 1

print(classification_report(labels_orig, preds_orig))
print("MAE:", mean_absolute_error(labels_orig, preds_orig))

#compute confusion matrix
cm = confusion_matrix(labels_orig, preds_orig)
classes = [1, 2, 3, 4, 5]

plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, cmap="Blues", fmt="d",
            xticklabels=classes, yticklabels=classes)

plt.xlabel("Predicted Rating")
plt.ylabel("Actual Rating")
plt.title("Confusion Matrix for Star Rating Prediction")
plt.show()